{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atharvanaik10/screentime/blob/main/ScreenTime.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TibOU0Pz9OfP"
      },
      "source": [
        "# Visualization of actor screentime in a given movie clip using deep learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyft2Fw4_QY4"
      },
      "source": [
        "## Project Goals:\n",
        "\n",
        "\n",
        "A neural network is trained to detect faces in a frame (image) and output the faces in the image. \n",
        "* We use `OpenCV` to extract individual frames from the movie clip\n",
        "* We use `MTCNN` (multitask cascaded convolutional neural network) to extract faces for each frame\n",
        "\n",
        "Another neural network is trained on a dataset of faces of Bollywood actors to recognize and classify faces. \n",
        "* We use `FaceNet` (wrapped in `keras`) to extract face mappings\n",
        "* We use `scikit-learn` to develop a classifier for our faces\n",
        "\n",
        "[More information about face recognition](https://docs.google.com/document/d/1598xceIfkDeDQmPRQhCrsC8KbZ0vLFTGVJUl3PAp99o/edit?usp=sharing)\n",
        "\n",
        "We use this classification of faces for each actor for each frame in the given movie clip to calculate the frequencies (# of frames) each actor appears in. \n",
        "* This is done using vanilla `Python` with `Numpy`\n",
        "\n",
        "We then visualize this data to show actor screentime percentage.\n",
        "* We use `Numpy` and `matplotlib` to compile and visualize data\n",
        "\n",
        "NOTE: To run this locally, you will have to change all absolute filepaths here to your respective filepaths.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywgkFOh4fW0I"
      },
      "source": [
        "## Imports\n",
        "0. Import necessary libraries and methods"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OXGspJlhuhyU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eSxWkA9I_ja",
        "outputId": "caf79dea-62e0-4a00-f412-746eadb59a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "Requirement already satisfied: mtcnn in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (4.1.2.30)\n",
            "Requirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.21.6)\n",
            "Requirement already satisfied: nolearn in /usr/local/lib/python3.7/dist-packages (0.6.1)\n",
            "Requirement already satisfied: Lasagne in /usr/local/lib/python3.7/dist-packages (from nolearn) (0.1)\n",
            "Requirement already satisfied: Theano in /usr/local/lib/python3.7/dist-packages (from nolearn) (1.0.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from nolearn) (1.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nolearn) (1.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from nolearn) (0.8.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from Lasagne->nolearn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nolearn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->nolearn) (3.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Theano->nolearn) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "!pip install mtcnn\n",
        "!pip install nolearn\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from tensorflow import keras\n",
        "import sklearn\n",
        "import sklearn.preprocessing\n",
        "import sklearn.svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isdir\n",
        "\n",
        "import PIL.Image\n",
        "import cv2\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsw10YXujLPj"
      },
      "source": [
        "## Face and feature extraction\n",
        "1. Extract faces from our dataset. We use a dataset of the top 100 bollywood actors: (https://www.kaggle.com/datasets/havingfun/100-bollywood-celebrity-faces)\n",
        "\n",
        "2. Use facenet to extract features from each class face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmDOeBt95JXi"
      },
      "outputs": [],
      "source": [
        "# extract a single face from a given photograph\n",
        "def extract_face(filename, required_size=(160, 160)):\n",
        "\t# load image from file\n",
        "\tpixels = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
        "\t# detect faces in the image\n",
        "\tresults = detector.detect_faces(pixels)\n",
        " \n",
        "\t# extract the bounding box from the first face\n",
        "\tx1, y1, width, height = results[0]['box']\n",
        "\tx1, y1 = abs(x1), abs(y1)\n",
        "\tx2, y2 = x1 + width, y1 + height\n",
        "\n",
        "\tface = pixels[y1:y2, x1:x2]\n",
        "\timage = PIL.Image.fromarray(face).resize(required_size)\n",
        "\tface_array = np.asarray(image)\n",
        " \n",
        "\treturn face_array\n",
        " \n",
        "# load images and extract a single face for all images in a directory\n",
        "def load_face(directory):\n",
        "\tfaces = []\n",
        "\tfail_count = 0\n",
        "\tfor filename in listdir(directory):\n",
        "\t\tpath = directory + filename\n",
        "\t\ttry:\n",
        "\t\t\tface = extract_face(path)\n",
        "\t\t\tfaces.append(face)\n",
        "\t\texcept:\n",
        "\t\t\tfail_count += 1\n",
        "\treturn faces, fail_count\n",
        " \n",
        "# load a dataset that contains one subdir for each class that in turn contains images\n",
        "def load_dataset(directory, test_size):\n",
        "\ttrainX, trainy = [],[]\n",
        "\ttestX, testy = [], []\n",
        "\n",
        "\tfor subdir in listdir(directory):\n",
        "\t\tpath = directory + '/' + subdir + '/'\n",
        "\t\t# skip any files that might be in the dir\n",
        "\t\tif not isdir(path):\n",
        "\t\t\tcontinue\n",
        "\t\t\n",
        "\t\t# load all faces in the subdirectory\n",
        "\t\tfaces, fail_count = load_faces(path)\n",
        "\t\tlabels = [subdir for idx in range(len(faces))]\n",
        "\n",
        "\t\t# summarize progress\n",
        "\t\tprint('> Loaded %d and failed %d faces for class: %s' % (len(faces), fail_count, subdir))\n",
        "\t\n",
        "\t\t# store the first [test_size] images as test data, rest as training data\n",
        "\t\ttestX.extend(faces[0:test_size])\n",
        "\t\ttesty.extend(labels[0:test_size])\n",
        "\t\ttrainX.extend(faces[test_size:])\n",
        "\t\ttrainy.extend(labels[test_size:])\n",
        "\t \n",
        "\treturn np.asarray(trainX), np.asarray(trainy), np.asarray(testX), np.asarray(testy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er7hDGYiguhC"
      },
      "outputs": [],
      "source": [
        "# create the detector, using default weights\n",
        "detector = MTCNN()\n",
        "\n",
        "# load dataset\n",
        "# TODO: relative filepath\n",
        "TEST_SIZE = 20\n",
        "trainX, trainy, testX, testy = load_dataset('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/subset_bollywood_faces', TEST_SIZE)\n",
        "\n",
        "print(trainX.shape, trainy.shape)\n",
        "print('Datasets loaded')\n",
        "\n",
        "# store faces\n",
        "pickle.dump(trainX, open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/trainX.pkl', 'wb'))\n",
        "pickle.dump(trainy, open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/trainy.pkl', 'wb'))\n",
        "pickle.dump(testX, open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/testX.pkl', 'wb'))\n",
        "pickle.dump(testy, open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/testy.pkl', 'wb'))\n",
        "print('Faces stored')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNhdbrY9UakR"
      },
      "outputs": [],
      "source": [
        "# unpickle faces\n",
        "trainX = pickle.load(open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/trainX.pkl', 'rb'))\n",
        "trainy = pickle.load(open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/trainy.pkl', 'rb'))\n",
        "testX = pickle.load(open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/testX.pkl', 'rb'))\n",
        "testy = pickle.load(open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/testy.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5U-un5qshP1_"
      },
      "outputs": [],
      "source": [
        "# calculate features using facenet\n",
        "\n",
        "def get_embedding(model, face):\n",
        "  face = face.astype('float32')\n",
        "  face = face-face.mean()/face.std()\n",
        "\n",
        "  yhat = model.predict(np.expand_dims(face, axis=0))\n",
        "  return yhat[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4DkKvdPiAIl",
        "outputId": "bc9c42d1-9bd4-41d8-a557-f9c67cc91dac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "# convert train and test data into feature arrays\n",
        "model = keras.models.load_model('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/keras-facenet/model/facenet_keras.h5')\n",
        "\n",
        "trainX_features = []\n",
        "testX_features = []\n",
        "\n",
        "for face in trainX:\n",
        "  trainX_features.append(get_embedding(model, face))\n",
        "trainX_features = np.asarray(trainX_features)\n",
        "\n",
        "for face in testX:\n",
        "  testX_features.append(get_embedding(model, face))\n",
        "testX_features = np.asarray(testX_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDIHsJ4GfMzY"
      },
      "source": [
        "## Training\n",
        "3. Train a classifier using train data and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRac_WD-ekWr"
      },
      "outputs": [],
      "source": [
        "# develop a classifier\n",
        "\n",
        "# normalize input vectors\n",
        "encoder = sklearn.preprocessing.Normalizer(norm='l2')\n",
        "\n",
        "# TODO: dump these\n",
        "trainX_features = encoder.transform(trainX_features)\n",
        "testX_features = encoder.transform(testX_features)\n",
        "\n",
        "# encode label targets (converts labels from strings to ints)\n",
        "# encoder = sklearn.preprocessing.LabelEncoder()\n",
        "# encoder.fit(trainy)\n",
        "# trainy_encoded = encoder.transform(trainy)\n",
        "# testy_encoded = encoder.transform(testy)\n",
        "\n",
        "# fit model\n",
        "classifier_model = sklearn.svm.SVC(C=10, kernel='rbf',probability=True, gamma=5)\n",
        "classifier_model.fit(trainX_features, trainy)\n",
        "\n",
        "# dump model\n",
        "pickle.dump(classifier_model, open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/classifier_model.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqrSvlcIhVHt",
        "outputId": "cecbdf18-a6d0-40bf-d610-9bdbb543397a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing complete with 0.64 accuracy\n"
          ]
        }
      ],
      "source": [
        "# test model on test data\n",
        "classifier_model = pickle.load(open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/classifier_model.pkl', 'rb'))\n",
        "\n",
        "preds = classifier_model.predict(testX_features)\n",
        "\n",
        "print(\"Testing complete with \" + str(sklearn.metrics.accuracy_score(testy, preds)) + \" accuracy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJgTymdujcRw"
      },
      "source": [
        "## Input processing\n",
        "4. Process input video clip into inout data with one frame from each second of video\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zu2Wb1_h2Lf"
      },
      "outputs": [],
      "source": [
        "def extract_frames(in_file, out_folder):\n",
        "  step = 1000 #in milliseconds\n",
        "\n",
        "  # read and store one frame from each second of video into out_folder \n",
        "  cap = cv2.VideoCapture(in_file)\n",
        "  success = True\n",
        "  frame = 0\n",
        "  success, image = cap.read()\n",
        "  while success:\n",
        "    cap.set(cv2.CAP_PROP_POS_MSEC, frame * step)\n",
        "    cv2.imwrite(out_folder + ('_%d.jpg' % frame), image)\n",
        "    frame += 1\n",
        "    success, image = cap.read()\n",
        "  return \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z093bvQNoj7X"
      },
      "outputs": [],
      "source": [
        "# for each video clip, store frames in a new folder\n",
        "in_file = \"/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/clips/clip2.mp4\"\n",
        "out_folder = \"/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/extracted_frames/clip2\"\n",
        "\n",
        "extract_frames(in_file, out_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SV8s0tnlqLDy"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7xQV7RWm0Qb"
      },
      "outputs": [],
      "source": [
        "# extract all faces from a given photograph\n",
        "def extract_faces(filename, required_size=(160, 160)):\n",
        "\tpixels = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\t# pixels = cv2.resize(pixels, (800, 800))\n",
        "\tfaces = detector.detect_faces(pixels)\n",
        "\tfaces_array = []\n",
        "\tfor idx in range(len(faces)):\n",
        "\t\t# extract the bounding box from the face\n",
        "\t\tx1, y1, width, height = faces[idx]['box']\n",
        "\t\tx1, y1 = abs(x1), abs(y1)\n",
        "\t\tx2, y2 = x1 + width, y1 + height\n",
        "\n",
        "\t\tface = pixels[y1:y2, x1:x2]\n",
        "\n",
        "\t\t# resize pixels to the model size\n",
        "\t\timage = PIL.Image.fromarray(face).resize(required_size)\n",
        "\n",
        "\t\tface_array = np.asarray(image)\n",
        "\t\tfaces_array.append(face_array)\n",
        "\n",
        "\treturn np.asarray(faces_array)\n",
        " \n",
        "# load images and extract faces for all images in a directory\n",
        "def load_faces(directory):\n",
        "\t# frame_faces of the form dict(): int frame -> faces[]\n",
        "\tframe_faces = {}\n",
        "\n",
        "\tfail_count = 0\n",
        "\tframe_count = 0\n",
        "\tfor filename in listdir(directory):\n",
        "\t\tpath = directory + filename\n",
        "\t\ttry:\n",
        "\t\t\tfaces = extract_faces(path)\n",
        "\t\t\tframe_faces[frame_count] = faces\n",
        "\t\texcept:\n",
        "\t\t\tfail_count += 1\n",
        "\t\t\tframe_faces[frame_count] = None\n",
        "\t\t# faces = extract_faces(path)\n",
        "\t\t# frame_faces[frame_count] = faces\n",
        "\t\tframe_count += 1\n",
        "\treturn frame_faces, fail_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJ4r0rUH5vF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da474d1d-e3fe-49d9-ae6c-1f65f03b5b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "87\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# for the folder of frames, get faces of each frame\n",
        "detector = MTCNN()\n",
        "classifier = pickle.load(open('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/pickle/classifier_model.pkl', 'rb'))\n",
        "frame_folder = \"/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/extracted_frames/\"\n",
        "\n",
        "\n",
        "frame_faces, fail_count = load_faces(frame_folder)\n",
        "print(len(frame_faces))\n",
        "print(fail_count)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(frame_faces[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAhQQcL-EIGj",
        "outputId": "5c1e83c2-34a8-4ca8-8cfb-6d0c92605bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 160, 160, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ17ZIZdSCX0",
        "outputId": "895f6a5e-ebf4-4f5f-e208-867244138b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "0 :  ['Kareena_Kapoor']\n",
            "1 :  ['Kareena_Kapoor']\n",
            "2 :  ['Shah_Rukh_Khan' 'Kareena_Kapoor' 'Hrithik_Roshan']\n",
            "3 :  ['Amitabh_Bachchan']\n",
            "4 :  ['Kajol']\n",
            "5 :  ['Hrithik_Roshan' 'Kareena_Kapoor']\n",
            "6 :  ['Hrithik_Roshan' 'Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "7 :  ['Hrithik_Roshan' 'Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "8 :  ['Hrithik_Roshan' 'Hrithik_Roshan' 'Kareena_Kapoor']\n",
            "9 :  ['Shah_Rukh_Khan']\n",
            "10 :  ['Shah_Rukh_Khan']\n",
            "11 :  ['Hrithik_Roshan']\n",
            "12 :  ['Shah_Rukh_Khan']\n",
            "13 :  ['Hrithik_Roshan']\n",
            "14 :  ['Hrithik_Roshan']\n",
            "15 :  ['Hrithik_Roshan']\n",
            "16 :  ['Hrithik_Roshan']\n",
            "17 :  ['Hrithik_Roshan']\n",
            "18 :  ['Hrithik_Roshan']\n",
            "19 :  ['Amitabh_Bachchan']\n",
            "20 :  ['Kajol']\n",
            "21 :  ['Kajol']\n",
            "22 :  ['Hrithik_Roshan']\n",
            "23 :  ['Kajol']\n",
            "24 :  ['Kareena_Kapoor' 'Hrithik_Roshan']\n",
            "25 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "26 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "27 :  ['Hrithik_Roshan' 'Amitabh_Bachchan']\n",
            "28 :  ['Hrithik_Roshan']\n",
            "29 :  ['Hrithik_Roshan']\n",
            "30 :  ['Hrithik_Roshan']\n",
            "31 :  ['Hrithik_Roshan']\n",
            "32 :  ['Hrithik_Roshan']\n",
            "33 :  ['Hrithik_Roshan']\n",
            "34 :  ['Kareena_Kapoor']\n",
            "35 :  ['Hrithik_Roshan']\n",
            "36 :  ['Hrithik_Roshan']\n",
            "37 :  ['Kajol']\n",
            "38 :  ['Hrithik_Roshan']\n",
            "39 :  ['Kareena_Kapoor' 'Hrithik_Roshan']\n",
            "40 :  ['Hrithik_Roshan' 'Kareena_Kapoor']\n",
            "41 :  ['Kareena_Kapoor' 'Kareena_Kapoor']\n",
            "42 :  ['Hrithik_Roshan' 'Kareena_Kapoor']\n",
            "43 :  ['Hrithik_Roshan' 'Kareena_Kapoor' 'Kareena_Kapoor']\n",
            "44 :  ['Hrithik_Roshan' 'Kareena_Kapoor' 'Kareena_Kapoor']\n",
            "45 :  ['Hrithik_Roshan' 'Kareena_Kapoor' 'Hrithik_Roshan']\n",
            "46 :  ['Kareena_Kapoor' 'Hrithik_Roshan' 'Kareena_Kapoor']\n",
            "47 :  ['Hrithik_Roshan' 'Kareena_Kapoor' 'Hrithik_Roshan']\n",
            "48 :  ['Hrithik_Roshan' 'Hrithik_Roshan' 'Kareena_Kapoor']\n",
            "49 :  ['Kajol']\n",
            "50 :  ['Kajol']\n",
            "51 :  ['Kajol']\n",
            "52 :  ['Hrithik_Roshan']\n",
            "53 :  ['Kajol']\n",
            "54 :  ['Kareena_Kapoor' 'Hrithik_Roshan']\n",
            "55 :  ['Kareena_Kapoor']\n",
            "56 :  ['Kareena_Kapoor' 'Hrithik_Roshan']\n",
            "57 :  ['Kajol']\n",
            "58 :  ['Amitabh_Bachchan' 'Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "59 :  ['Hrithik_Roshan' 'Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "60 :  ['Kareena_Kapoor' 'Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "61 :  ['Kareena_Kapoor']\n",
            "62 :  ['Shah_Rukh_Khan']\n",
            "64 :  ['Shah_Rukh_Khan' 'Hrithik_Roshan']\n",
            "65 :  ['Hrithik_Roshan']\n",
            "66 :  ['Amitabh_Bachchan' 'Hrithik_Roshan']\n",
            "67 :  ['Kareena_Kapoor' 'Hrithik_Roshan']\n",
            "68 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "69 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "70 :  ['Shah_Rukh_Khan']\n",
            "71 :  ['Kareena_Kapoor']\n",
            "72 :  ['Hrithik_Roshan']\n",
            "73 :  ['Hrithik_Roshan']\n",
            "74 :  ['Hrithik_Roshan']\n",
            "75 :  ['Hrithik_Roshan']\n",
            "76 :  ['Hrithik_Roshan']\n",
            "77 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "78 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "79 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "80 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "81 :  ['Hrithik_Roshan' 'Hrithik_Roshan']\n",
            "82 :  ['Hrithik_Roshan']\n",
            "83 :  ['Hrithik_Roshan']\n",
            "84 :  ['Hrithik_Roshan']\n",
            "85 :  ['Hrithik_Roshan']\n",
            "86 :  ['Kajol']\n"
          ]
        }
      ],
      "source": [
        "model = keras.models.load_model('/gdrive/MyDrive/Sem4/CWL207/2022_207_Group_21/dataset/keras-facenet/model/facenet_keras.h5')\n",
        "encoder = sklearn.preprocessing.Normalizer(norm='l2')\n",
        "\n",
        "frame_preds = {}\n",
        "# for each frame\n",
        "for key in frame_faces:\n",
        "  if(len(frame_faces[key] != 0)):\n",
        "    embedded = []\n",
        "    for face in frame_faces[key]:\n",
        "      embedded.append(get_embedding(model, face))\n",
        "    embedded = np.asarray(embedded)\n",
        "    embedded = encoder.transform(embedded)\n",
        "    preds = classifier.predict(embedded)\n",
        "    frame_preds[key] = preds\n",
        "    print(str(key) ,\": \", preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T07e8yqIqORv"
      },
      "source": [
        "## Analysis and visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "id": "ywAGcJOg6Hc6",
        "outputId": "98b95b46-22a3-4ade-c3ad-9d21ee8ace9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Kareena_Kapoor': 28, 'Shah_Rukh_Khan': 7, 'Hrithik_Roshan': 82, 'Amitabh_Bachchan': 5, 'Kajol': 11}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 5 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# graph frequencies using matplotlib\n",
        "\n",
        "frequencies = {}\n",
        "\n",
        "for key in frame_preds:\n",
        "  for actor in frame_preds[key]:\n",
        "    try:\n",
        "      frequencies[actor] += 1\n",
        "    except KeyError:\n",
        "      frequencies[actor] = 1\n",
        "\n",
        "print(frequencies)\n",
        "# df = pandas.DataFrame.from_dict(frequencies)\n",
        "# df.plot.bar\n",
        "plt.bar(frequencies.keys(), frequencies.values(), 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis and next steps:\n",
        "\n",
        "- We have successfully demonstrated, as a proof of concept, the ability to extract the frequencies that actors appear in a given clip\n",
        "\n",
        "- Our classifier model, unfortunately, has a low accuracy of ~76% on test data, because of the fact that most classifiers are trained on caucasian faces. Because of this, South Asian faces become hard to extract and recognize\n",
        "\n",
        "- Next steps include working to create better classification models, using more training data, and testing this with longer clips. "
      ],
      "metadata": {
        "id": "cNwNuBpyrev1"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}